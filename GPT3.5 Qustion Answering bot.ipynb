{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Zz0fQin3fI",
        "outputId": "b64770aa-18f4-4f3b-a335-314ab622d6e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Collecting PyMuPDFb==1.24.3 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: PyMuPDFb, pymupdf, openai\n",
            "Successfully installed PyMuPDFb-1.24.3 openai-0.28.0 pymupdf-1.24.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##My approach\n",
        "\n",
        "##First, we use OpenAI's embedding model to create word embeddings from the text extracted from a PDF. When a user asks a question, we identify text chunks that closely match the query using cosine similarity. These relevant chunks are then fed as context to OpenAI's GPT model, which generates the answers based on this context. This process supports continuous queries, storing all Q&A pairs in a JSON file upon exit."
      ],
      "metadata": {
        "id": "ScnXRxMcus4Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09_Knwqkns_H",
        "outputId": "74e9bb06-3e62-47d0-cc42-5f17e950ab74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please type your question (or 'exit' to finish): What is prompt management? why do we need it?\n",
            "Answer: Prompt management is the process of storing and testing multiple prompts in order to simplify the prompt evaluation process. It is essential because developers often need to evaluate and make changes to prompts multiple times during the development and testing phases. Without a prompt management process in place, developers may have to recreate past prompts, leading to increased development time and potential errors. By implementing a prompt management process, developers can streamline the testing and evaluation processes, allowing for easier comparison of multiple prompts and variations.\n",
            "Please type your question (or 'exit' to finish): exit\n",
            "All questions and answers have been saved to qa_pairs.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import fitz\n",
        "import openai\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "def extract_text_from_pdf(file_path: str):\n",
        "    text = \"\"\n",
        "    pdf_document = fitz.open(file_path)\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document[page_num]\n",
        "        text += page.get_text()\n",
        "    return text.replace(\"\\n\", \" \")\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 128, chunk_overlap: int = 20):\n",
        "    words = text.split(\" \")\n",
        "    words = [word for word in words if word]\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - chunk_overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def get_embedding(text: str, api_key: str, model: str = \"text-embedding-ada-002\"):\n",
        "    openai.api_key = api_key\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = openai.Embedding.create(input=[text], model=model)\n",
        "    return np.array(response[\"data\"][0][\"embedding\"])\n",
        "\n",
        "def index_document(chunks: List[str], api_key: str, model: str = \"text-embedding-ada-002\"):\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        embedding = get_embedding(chunk, api_key, model)\n",
        "        embeddings.append(embedding)\n",
        "    return chunks, np.vstack(embeddings)\n",
        "\n",
        "def cosine_similarity(query_embedding: np.ndarray, embeddings_matrix: np.ndarray):\n",
        "    norms = np.linalg.norm(embeddings_matrix, axis=1) * np.linalg.norm(query_embedding)\n",
        "    similarities = np.dot(embeddings_matrix, query_embedding) / norms\n",
        "    return similarities\n",
        "\n",
        "def get_top_k_chunks(query: str, indexed_chunks: List[str], embeddings_matrix: np.ndarray, api_key: str, model: str, k: int = 5):\n",
        "    query_embedding = get_embedding(query, api_key, model)\n",
        "    similarities = cosine_similarity(query_embedding, embeddings_matrix)\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
        "    return [indexed_chunks[i] for i in top_k_indices]\n",
        "\n",
        "def generate_answer(query: str, context: str, api_key: str, model: str = \"gpt-3.5-turbo\"):\n",
        "    openai.api_key = api_key\n",
        "    prompt = f\"Context:\\n{context}\\n\\nAnswer the question based on the above context:\\n{query}\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def doc_qa_from_pdf(file_path: str, api_key: str, embedding_model: str = \"text-embedding-ada-002\", completion_model: str = \"gpt-3.5-turbo\"):\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "    chunks = chunk_text(text)\n",
        "    indexed_chunks, embeddings_matrix = index_document(chunks, api_key, embedding_model)\n",
        "    return {\"indexed_chunks\": indexed_chunks, \"embeddings_matrix\": embeddings_matrix, \"api_key\": api_key, \"embedding_model\": embedding_model, \"completion_model\": completion_model}\n",
        "\n",
        "def answer_question(docqa: Dict, question: str):\n",
        "    top_chunks = get_top_k_chunks(question, docqa[\"indexed_chunks\"], docqa[\"embeddings_matrix\"], docqa[\"api_key\"], docqa[\"embedding_model\"])\n",
        "    context = \"\\n\".join(top_chunks)\n",
        "    return generate_answer(question, context, docqa[\"api_key\"], docqa[\"completion_model\"])\n",
        "\n",
        "def main_loop(file_path: str, api_key: str, embedding_model: str, completion_model: str):\n",
        "    docqa = doc_qa_from_pdf(file_path, api_key, embedding_model, completion_model)\n",
        "    qa_pairs = []\n",
        "\n",
        "    while True:\n",
        "        question = input(\"Please type your question (or 'exit' to finish): \")\n",
        "        if question.lower() == \"exit\":\n",
        "            break\n",
        "        answer = answer_question(docqa, question)\n",
        "        print(f\"Answer: {answer}\")\n",
        "        qa_pairs.append({\"question\": question, \"answer\": answer , \"model\":completion_model})\n",
        "\n",
        "    with open(\"qa_pairs.json\", \"w\") as f:\n",
        "        json.dump(qa_pairs, f, indent=4)\n",
        "    print(\"All questions and answers have been saved to qa_pairs.json\")\n",
        "\n",
        "\n",
        "##As this is modular, you can try multiple models. Just ensure to switch it up with your API key\n",
        "\n",
        "##To exit the QA session type \"exit\" in the prompt ,\n",
        "\n",
        "##Finally a json file is generated with the Question-Answer pair and the model name\n",
        "\n",
        "main_loop(file_path=\"GenAI_Handbook.pdf\",\n",
        "          api_key = \"\",\n",
        "          embedding_model = \"text-embedding-ada-002\",\n",
        "          completion_model = \"gpt-3.5-turbo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YaL5xhppcrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}